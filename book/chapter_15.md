# ГЛАВА 15

#### [Левая страница]

```text
// CHAPTER 15 ___________________________________________
// TRAINING

> СУТЬ (STATUS):
СЖАТИЕ ИНТЕРНЕТА.
Тренировка LLM — это попытка сжать весь текст интернета
в один файл (архив), но с потерями. Модель учится
восстанавливать информацию, а не хранить её.

> ПОЧЕМУ ЭТО ВАЖНО (WHY IT MATTERS):
 1. Ресурсы: Тренировка топовой модели стоит $100M+
    и требует 10,000 GPU. Это спорт для гигантов.
 2. Датасеты: Качество модели зависит от того, что она
    читала. "Мусор на входе — мусор на выходе".
 3. Cutoff Date: Модель ничего не знает о событиях,
    произошедших после окончания тренировки.
```

---

#### [Правая страница]

**ПОГРУЖЕНИЕ:**

Представьте, что вы хотите заархивировать всю Википедию, но у вас есть флешка только на 1 ГБ.
Вы не можете сохранить текст дословно.
Вам придется сохранить **закономерности**.

Вместо того чтобы хранить фразу "Лондон — столица Великобритании", модель запоминает связь:
`Столица + Страна -> Город`.

Тренировка — это поиск таких закономерностей.
Модель читает текст, прячет одно слово и пытается его угадать.
Сначала она угадывает случайно. Но через триллион попыток она начинает понимать грамматику, логику, факты и даже юмор.

**Loss Function (Функция потерь):**
Это линейка, которой бьют модель по рукам.
Чем сильнее ошибка предсказания, тем больнее модели (выше Loss).
Цель тренировки — свести Loss к минимуму.

Результат тренировки — это **zip-архив знаний человечества**, который умеет распаковываться по запросу.
Но иногда он распаковывается с ошибками (галлюцинациями), потому что сжатие было слишком сильным.

```text
       THE COMPRESSION RATIO
       
       [ THE INTERNET ]
       Size: ~100 Petabytes
       Content: All books, code, articles, spam.
       
              |
              v (Training / Compression)
              |
       
       [ THE MODEL (GPT-4) ]
       Size: ~100 Gigabytes (Weights)
       Content: Abstract patterns of logic and language.
       
       
       RATIO: 1 : 1,000,000
       It's not a database. It's a hologram.
```
