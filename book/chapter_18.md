# ГЛАВА 18

#### [Левая страница]

```text
// CHAPTER 18 ___________________________________________
// ATTENTION IS ALL YOU NEED

> СУТЬ (STATUS):
МЕХАНИЗМ ВНИМАНИЯ.
Технология, которая изменила всё (2017). Она позволяет
модели фокусироваться на важных словах в предложении,
игнорируя шум.

> ПОЧЕМУ ЭТО ВАЖНО (WHY IT MATTERS):
 1. Контекст: Благодаря Attention модель понимает, что в фразе
    "Банк выдал деньги" слово "Банк" — это финансы, а не река.
 2. Память: Attention связывает начало длинного текста с концом.
 3. Трансформер: Это архитектура, на которой построены все
    современные LLM (GPT = Generative Pre-trained Transformer).
```

---

#### [Правая страница]

**ПОГРУЖЕНИЕ:**

До 2017 года нейросети читали текст как люди: слово за словом, слева направо.
К концу длинного предложения они забывали начало.

Google придумал **Self-Attention**.
Модель читает ВСЕ слова одновременно.
И каждое слово начинает "смотреть" на своих соседей, спрашивая: "Мы связаны?"

Фраза: *"Маша съела кашу, потому что она была голодна".*

Слово **"она"** смотрит вокруг:
*   Связано ли я с "кашей"? (Слабо).
*   Связано ли я с "Машей"? (Сильно!).

Вектор слова "она" вбирает в себя смысл слова "Маша".
Теперь для модели "она" = "Маша-голодная".

Это создает паутину смысловых связей.
Чем длиннее текст, тем сложнее паутина (квадратичная сложность).
Поэтому контекстное окно дорого стоит. Удвоение длины текста требует в 4 раза больше вычислений внимания.

```text
       ATTENTION MAP
       
       "The animal didn't cross the street because it was too tired."
       
       Target: [ IT ]
       
       Looking at:
       [ The ] .... 0%
       [ animal ] .......................... 90% (LINK FOUND!)
       [ street ] . 5%
       [ tired ] .. 5%
       
       Result:
       IT = ANIMAL (not street).
```
