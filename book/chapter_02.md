# Глава 2. Язык машин

## Разговор с бесконечностью

Мы привыкли общаться с людьми, используя интонации, жесты и контекст. Но когда мы говорим с машиной, мы вступаем в мир строгой логики и математической вероятности.
Чтобы стать архитектором AI-систем, нужно понять, как «думает» модель, и на каком языке с ней говорить.

## API: Цифровой нейроинтерфейс

Для обычного пользователя ИИ — это чат в браузере. Вы пишите «Привет», и получаете ответ. Но для инженера ИИ — это **API (Application Programming Interface)**.
Это способ, которым программы общаются друг с другом.

Представьте, что вы в ресторане. Вы (клиент) не идете на кухню (сервер) готовить еду. Вы даете заказ официанту (API). Официант относит заказ, кухня готовит, официант приносит результат.
В мире ИИ «заказ» оформляется в формате **JSON**. Это выглядит примерно так:

```json
{
  "role": "user",
  "content": "Скажи привет!"
}
```

Это и есть язык, который понимает машина. Строгая структура, где каждому ключу соответствует значение.

## Иллюзия памяти

Одна из самых важных вещей, которую нужно понять: **LLM не имеет памяти**.
Каждый раз, когда вы отправляете запрос, для модели это происходит впервые. Она не помнит, о чем вы говорили 5 минут назад.
Как же тогда работает чат?
Мы каждый раз отправляем модели **всю историю переписки**.
Мы посылаем: «Я сказал А, ты ответил Б, я говорю В... что ты ответишь?»
Это называется «контекст». И он ограничен. Это как оперативная память — она не бесконечна.

Это свойство называется **Stateless** (без сохранения состояния). Модель как золотая рыбка — каждый новый запрос для нее начинается с чистого листа, если вы сами не напомните ей о прошлом.

## Настройка «мозга»

У модели есть рычаги управления. Самый главный — **Temperature** (Температура).
Это параметр случайности.
*   Если температура 0, модель всегда будет выбирать самое вероятное следующее слово. Она станет скучной, предсказуемой, но точной. Идеально для кода или фактов.
*   Если температура 1, модель начнет рисковать, выбирая менее вероятные слова. Это добавляет «креативности», но повышает риск бреда.

Есть и другие фильтры, такие как **Top-P** и **Top-K**, которые отсекают слишком маловероятные варианты, не давая модели уйти в полный хаос.

## Цена интеллекта

Современные модели требуют колоссальных ресурсов. Чтобы запустить «умную» модель, нужны видеокарты стоимостью в сотни тысяч долларов.
Но инженеры нашли выход — **Квантизация**.
Представьте число Пи: 3.14159265... Хранить его целиком дорого. А если мы округлим до 3.14? Смысл почти не поменялся, а места занимает меньше.
То же самое делают с «весами» нейросети. Уменьшая точность хранения чисел, мы можем запускать мощные модели даже на домашних компьютерах, почти не теряя в качестве.

## Промпт-инжиниринг: Искусство вопроса

В мире ИИ качество ответа зависит от качества вопроса.
Существуют стратегии:
1.  **Zero-shot**: Просто спросить. («Переведи это»).
2.  **Few-shot**: Дать примеры. («Яблоко -> red, Банан -> yellow, Огурец -> ...»). Это резко повышает точность.
3.  **Persona**: «Ты опытный юрист с 20-летним стажем...». Это задает модели ролевую модель поведения.

Но самое интересное начинается, когда мы позволяем модели не просто говорить, а **действовать**.

## От слов к делу: Function Calling

Изначально LLM могли только генерировать текст. Но что, если модель захочет узнать погоду? Она не может сама выйти в интернет.
Здесь появляется **Function Calling**.
Мы можем сказать модели: «У тебя есть инструмент `get_weather(city)`. Если тебе нужно узнать погоду, дай мне знать».
И когда вы спросите: «Какая погода в Лондоне?», модель не станет выдумывать. Она вернет специальный код: «Вызови `get_weather` для города Лондон».
Ваша программа выполнит этот код, получит данные и вернет их модели.

Это и есть момент рождения **Агента**. Модель перестает быть просто болтуном. Она получает руки, которыми может взаимодействовать с миром.
