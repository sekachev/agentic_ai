# ГЛАВА 14

#### [Левая страница]

```text
// CHAPTER 14 ___________________________________________
// THE WEIGHTS

> СУТЬ (STATUS):
ЗАМОРОЖЕННЫЙ МОЗГ.
Веса (Parameters) — это знания модели. Это триллионы чисел,
которые определяют силу связи между нейронами.
После обучения они не меняются (статичны).

> ПОЧЕМУ ЭТО ВАЖНО (WHY IT MATTERS):
 1. Обучение vs Работа: Обучение — это поиск весов.
    Работа (Inference) — это использование найденных весов.
 2. Статичность: GPT-4 не учится, когда вы с ней говорите.
    Она не запоминает ваши слова "навсегда". Веса неизменны.
 3. Размер: "70B модель" означает 70 миллиардов весов.
    Чем больше весов, тем умнее (и медленнее) модель.
```

---

#### [Правая страница]

**ПОГРУЖЕНИЕ:**

Представьте мозг как огромную сеть труб, по которым течет вода (информация).
На каждом соединении труб стоит кран (вентиль).
Этот кран можно приоткрыть (вес 0.9) или почти закрыть (вес 0.1).

**Веса — это положение этих вентилей.**
Когда мы "обучаем" модель, мы прогоняем через неё текст и крутим вентили так, чтобы на выходе получался правильный ответ.
Это занимает месяцы и требует атомную электростанцию энергии.

Но как только обучение закончено, вентили **завариваются намертво**.
Модель превращается в файл.
Когда вы скачиваете `Llama-3-70b.gguf`, вы скачиваете слепок положения этих вентилей.

**Почему модель не учится в процессе диалога?**
Потому что менять веса на лету слишком дорого и сложно.
Когда вы говорите модели "Запомни, меня зовут Алекс", она не меняет свои нейроны. Она просто держит "Алекс" в **Контекстном Окне** (краткосрочной памяти).
Как только вы закроете чат, "Алекс" исчезнет. Веса останутся прежними.

```text
       NEURAL CONNECTION
       
       [ Neuron A ] --(Weight: 0.5)--> [ Neuron B ]
                    --(Weight: 0.1)--> [ Neuron C ]
       
       
       TRAINING:
       "Adjustment Phase"
       Change 0.5 -> 0.6
       Change 0.1 -> 0.01
       (Expensive, Slow)
       
       INFERENCE:
       "Usage Phase"
       Weights are READ-ONLY.
       Signal flows through fixed paths.
       (Cheap, Fast)
```
