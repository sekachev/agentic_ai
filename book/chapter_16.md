# ГЛАВА 16

#### [Левая страница]

```text
// CHAPTER 16 ___________________________________________
// INFERENCE

> СУТЬ (STATUS):
ЗАПУСК РАЗУМА.
Инференс — это процесс использования уже обученной модели
для генерации ответов. Это то, что происходит, когда вы
нажимаете кнопку "Send".

> ПОЧЕМУ ЭТО ВАЖНО (WHY IT MATTERS):
 1. Стоимость: Тренировка платится один раз. Инференс
    платится за каждый запрос. Это основные расходы.
 2. Скорость: Мы хотим мгновенный ответ. Инференс требует
    мощных GPU с быстрой памятью (VRAM).
 3. Локальность: Инференс можно запускать на своем ноутбуке,
    если модель влезает в память.
```

---

#### [Правая страница]

**ПОГРУЖЕНИЕ:**

Модель — это как партитура симфонии (Веса).
Инференс — это исполнение этой симфонии оркестром (GPU).

Процесс выглядит так:
1.  **Промпт:** "Привет!" (Токены: `[153, 2094]`)
2.  **Загрузка:** Веса модели (40 ГБ) загружаются в видеопамять.
3.  **Прогон:** Токены летят сквозь слои нейросети. Происходят миллиарды умножений матриц.
4.  **Вывод:** Выпадает токен `[834]` ("Как").
5.  **Цикл:** Токен `[834]` добавляется к входу, и всё повторяется.
    "Привет! Как" -> ... -> "дела".

**Почему это дорого?**
Чтобы родить ОДНО слово, компьютер должен перелопатить ВСЕ 70 миллиардов параметров модели.
Это как прочитать всю Британскую энциклопедию, чтобы ответить "Да".

Поэтому скорость инференса измеряется в **Токенах в секунду (Tk/s)**.
Человек читает 5-10 Tk/s.
Хорошая модель генерирует 50-100 Tk/s.
Мы хотим, чтобы машина говорила быстрее, чем мы успеваем читать.

```text
       INFERENCE PIPELINE
       
       User: "2+2="
               |
               v
       [ GPU Computing... ] (Billions of operations)
               |
               v
       Output: "4" (Token generated)
               |
       (Loop back) -> Input: "2+2=4"
               |
               v
       [ GPU Computing... ]
               |
               v
       Output: "<EOS>" (End of Sequence)
```
