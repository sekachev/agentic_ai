# Модуль 1. Часть 2: Понятия ИИ (AI Concepts)

## Остров 1: Великая развилка (Deterministic vs ML)
*Цель: Показать фундаментальный сдвиг от написания правил к обучению на данных.*

### Слайд 1.1: Конец эпохи инструкций
*   **Заголовок:** Великая развилка: Конец эпохи инструкций
*   **Текст:**
    *   **Программирование 1.0 (Детерминизм):** «Если X, то Y». Полный контроль, нулевая гибкость.
    *   **Программирование 2.0 (Deep Learning):** Мы даем примеры, а модель сама находит закономерности.
    *   Машина перестает быть калькулятором и становится «учеником».
*   **Иллюстрация:** Сравнение блок-схемы (алгоритм) и сложного графа нейросети.
*   **Источник:** Wikipedia — [Flowchart](https://en.wikipedia.org/wiki/File:LampFlowchart.svg) vs [Neural Network](https://en.wikipedia.org/wiki/File:Artificial_neural_network.svg)

### Слайд 1.2: Проблема «простого кота»
*   **Заголовок:** Почему нельзя запрограммировать кота?
*   **Текст:**
    *   **Задача:** Написать алгоритм распознавания кошек.
    *   **Провал классики:** 
        *   Уши? (Бывают без ушей). 
        *   Хвост? (Бывают без хвоста). 
        *   Шерсть? (Сфинксы).
    *   **Вердикт:** Человеческий язык и код слишком бедны, чтобы описать бесконечность реального мира.
*   **Иллюстрация:** Коллаж из Wikipedia (разные породы и позы котов).
*   **Источник:** [Wikipedia: Cat breeds](https://en.wikipedia.org/wiki/File:Domestic_Cat_Face_Side.jpg)

### Слайд 1.3: Смена парадигмы (Software 2.0)
*   **Заголовок:** Из архитекторов в садовники
*   **Текст:**
    *   **Раньше:** Мы проектировали каждую деталь системы.
    *   **Теперь (ML):** Мы проектируем только «способность учиться» и готовим данные.
    *   Логика модели распределена в триллионах чисел (весах), которые никто не писал руками.
*   **Иллюстрация:** Фото классического перфокарточного зала vs современная ферма GPU.

### Слайд 1.4: Главный контраст: Apollo против Tesla
*   **Заголовок:** Код рук человеческих vs Код ИИ
*   **Текст:**
    *   **Apollo Guidance Computer (1969):** 145 тыс. строк. Проверено жизнью людей. Машина для одной цели.
    *   **Tesla FSD (2024):** End-to-End нейросети. Машина «видит» и «принимает решения», основываясь на опыте миллионов водителей.
    *   **Вывод:** На смену точным формулам пришла «вероятностная интуиция».
*   **Иллюстрация:** Маргарет Гамильтон рядом с кодом Apollo.
*   **Источник:** [Wikipedia: Margaret Hamilton](https://en.wikipedia.org/wiki/File:Margaret_Hamilton_in_1969.jpg)

---

## Остров 2: Черный ящик (Что внутри LLM?)
*Цель: Разобрать устройство модели от простого примера к архитектуре.*

### Слайд 2.1: Т9 на стероидах
*   **Заголовок:** ИИ — это предсказание будущего (на 1 шаг за раз)
*   **Текст:**
    *   **Пример:** «Мама мыла...» -> (раму, посуду, окно?).
    *   Суть любой LLM: Какое слово (токен) будет следующим с наибольшей вероятностью?
    *   Весь «разум» ChatGPT — это умение невероятно точно угадывать продолжение текста.
*   **Иллюстрация:** Скриншот автодополнения в телефоне или поисковой строке.

### Слайд 2.2: История: От Тьюринга до Трансформеров
*   **Заголовок:** Короткий путь к прорыву
*   **Текст:**
    *   **1950:** Тест Тьюринга (может ли машина имитировать человека?).
    *   **1980-е:** Первые нейросети и «зимы ИИ».
    *   **2017:** Статья «Attention Is All You Need». Рождение архитектуры Transformer — фундамента современных LLM.
*   **Иллюстрация:** Обложка научной статьи "Attention Is All You Need".
*   **Источник:** [ArXiv: Attention Is All You Need](https://arxiv.org/abs/1706.03762)

### Слайд 2.3: Токены — язык, который понимает машина
*   **Заголовок:** Как ИИ «читает» текст
*   **Текст:**
    *   Модели не видят буквы. Они видят **токены** (кусочки слов).
    *   `Apple` = 1 токен. `Friendship` = 2-3 токена.
    *   Словарь модели — это набор пронумерованных фрагментов (как детальки LEGO).
*   **Иллюстрация:** Пример разбивки предложения на цветные блоки-токены (Tokenizer).

### Слайд 2.4: Эмбеддинги: Смысл как координаты
*   **Заголовок:** Геометрия смыслов
*   **Текст:**
    *   Математика превращает слова в векторы (списки чисел).
    *   **Король - Мужчина + Женщина = Королева.**
    *   В пространстве ИИ похожие понятия (кошка и собака) находятся физически рядом.
*   **Иллюстрация:** 3D-график с облаком точек, где слова сгруппированы по смыслу.

### Слайд 2.5: Слои и Параметры (Масштаб)
*   **Заголовок:** Сколько «мозгов» у нейросети?
*   **Текст:**
    *   **Веса (параметры):** Это «ручки настройки» нейронов.
    *   **GPT-3:** 175 миллиардов параметров.
    *   **GPT-4:** По слухам, триллионы.
    *   Чем больше параметров и слоев, тем более тонкие нюансы мира модель может уловить.
*   **Иллюстрация:** Схема многослойной нейронной сети с миллионами связей.
