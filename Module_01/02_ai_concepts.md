# Модуль 1. Часть 2: Понятия ИИ (AI Concepts)

## Остров 1: Великая развилка (Deterministic vs ML)
*Цель: Показать фундаментальный сдвиг от написания правил к обучению на данных.*

### Слайд 1.1: Конец эпохи инструкций
*   **Заголовок:** Великая развилка: Конец эпохи инструкций
*   **Текст:**
    *   **Программирование 1.0 (Детерминизм):** «Если X, то Y». Полный контроль, нулевая гибкость.
    *   **Программирование 2.0 (Machine Learning):** Мы даем примеры, а модель сама находит закономерности.
    *   Машина перестает быть калькулятором и становится «учеником».
*   **Иллюстрация:**
    ![Flowchart](https://upload.wikimedia.org/wikipedia/commons/9/91/LampFlowchart.svg)
    ![Neural Network](https://upload.wikimedia.org/wikipedia/commons/4/46/Colored_neural_network.svg)

### Слайд 1.2: Проблема «простого кота»
*   **Заголовок:** Почему нельзя запрограммировать кота?
*   **Текст:**
    *   **Задача:** Написать алгоритм распознавания кошек.
    *   **Провал классики:** 
        *   Уши? (Бывают без ушей). 
        *   Хвост? (Бывают без хвоста). 
        *   Шерсть? (Сфинксы).
    *   **Вердикт:** Человеческий язык и код слишком бедны, чтобы описать бесконечность реального мира.
*   **Иллюстрация:**
    ![Different Cats](https://upload.wikimedia.org/wikipedia/commons/b/bb/Kittyply_edit1.jpg)

### Слайд 1.3: Смена парадигмы (Software 2.0)
*   **Заголовок:** Из архитекторов в садовники
*   **Текст:**
    *   **Раньше:** Мы проектировали каждую деталь системы.
    *   **Теперь (ML):** Мы проектируем только «способность учиться» и готовим данные.
    *   Логика модели распределена в триллионах чисел (весах), которые никто не писал руками.
*   **Иллюстрация:**
    ![Old Data Center](https://upload.wikimedia.org/wikipedia/commons/c/cc/US_Census_Bureau_computer_room_1980s.jpg)

### Слайд 1.4: Главный контраст: Apollo против Tesla
*   **Заголовок:** Код рук человеческих vs Код ИИ
*   **Текст:**
    *   **Apollo Guidance Computer (1969):** 145 тыс. строк. Проверено жизнью людей. Каждая логическая связь была детерминирована.
    *   **Tesla FSD (2024):** End-to-End нейросети. Машина «видит» и «принимает решения», основываясь на опыте миллионов водителей.
    *   **Вывод:** На смену точным формулам пришла «вероятностная интуиция».
*   **Иллюстрация:** Маргарет Гамильтон рядом с кодом Apollo.
    ![Margaret Hamilton](https://upload.wikimedia.org/wikipedia/commons/d/db/Margaret_Hamilton_-_restoration.jpg)

---

## Остров 2: Черный ящик (Что внутри LLM?)
*Цель: Разобрать устройство модели от простого примера к архитектуре.*

### Слайд 2.1: Т9 на стероидах
*   **Заголовок:** ИИ — это предсказание будущего (на 1 шаг за раз)
*   **Текст:**
    *   **Пример:** «Мама мыла...» -> (раму, посуду, окно?).
    *   Суть любой LLM: Какое слово (токен) будет следующим с наибольшей вероятностью?
    *   Весь «разум» ChatGPT — это умение невероятно точно угадывать продолжение текста.
*   **Иллюстрация:**
    ![T9 Predict](https://upload.wikimedia.org/wikipedia/commons/2/23/Nokia_3310_blue.jpg)

### Слайд 2.2: История: От Тьюринга до Трансформеров
*   **Заголовок:** Короткий путь к прорыву
*   **Текст:**
    *   **1950:** Тест Тьюринга.
    *   **1980-е:** Первые нейросети.
    *   **2017:** Статья «Attention Is All You Need». Рождение архитектуры Transformer — фундамента современных LLM.
*   **Иллюстрация:** Архитектура трансформатора.
    ![Transformer Architecture](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)

### Слайд 2.3: Токены — язык, который понимает машина
*   **Заголовок:** Как ИИ «читает» текст
*   **Текст:**
    *   Модели не видят буквы. Они видят **токены** (кусочки слов).
    *   `Apple` = 1 токен. `Friendship` = 2-3 токена.
    *   Словарь модели — это набор пронумерованных фрагментов.
*   **Иллюстрация:** Визуализация токенизации (цветные блоки текста).

### Слайд 2.4: Эмбеддинги: Смысл как координаты
*   **Заголовок:** Геометрия смыслов
*   **Текст:**
    *   Математика превращает слова в векторы (списки чисел).
    *   **Король - Мужчина + Женщина = Королева.**
    *   В пространстве ИИ похожие понятия находятся физически рядом.
*   **Иллюстрация:** 3D-график с облаком точек (Word Embeddings).

---

## Остров 3: Кузница Разума (Training — Всеядность)
*Цель: Показать масштаб данных и процесс поглощения знаний.*

### Слайд 3.1: Великое пожирание данных
*   **Заголовок:** Всеядность: Как модель «съедает» интернет
*   **Текст:**
    *   Для обучения используются терабайты текста (Common Crawl).
    *   **Меню модели:** Wikipedia, Reddit, оцифрованные книги, код с GitHub, научные статьи.
    *   **Проблема авторства:** Модель учится на всём, что когда-либо создал человек.
*   **Иллюстрация:** Визуализация объема данных или логотипы основных источников данных.

### Слайд 3.2: Обучаем без учителя (Self-supervised)
*   **Заголовок:** Как ИИ учится сам?
*   **Текст:**
    *   Мы не объясняем модели каждое слово.
    *   **Метод «Пятен»:** Мы закрываем случайное слово в предложении и заставляем модель его угадать.
    *   Повторите это **триллион** раз — и модель начнет понимать логику языка, физики и общения.
*   **Иллюстрация:** Пример текста с пропущенными словами (fill-in-the-blanks).

### Слайд 3.3: Выращивание интеллекта (Compute)
*   **Заголовок:** Завод по производству знаний
*   **Текст:**
    *   Обучение — это не «программирование», а вычислительный шторм.
    *   Тысячи GPU (H100) работают месяцами.
    *   **Цена вопроса:** Десятки и сотни миллионов долларов за одну модель.
*   **Иллюстрация:** Современная серверная стойка с GPU-ускорителями.

---

## Остров 4: Момент Истины (Inference)
*Цель: Что происходит здесь и сейчас, когда мы пишем промпт.*

### Слайд 4.1: Магия Inference
*   **Заголовок:** Inference: Когда модель начинает «думать»
*   **Текст:**
    *   После обучения веса модели «замораживаются». Она больше не учится в процессе диалога.
    *   Промпт — это **начало координат**. Модель просто продолжает ваш текст на основе вероятностей.
    *   **Inference** — это процесс вытягивания знаний из огромной замороженной матрицы.
*   **Иллюстрация:** Схема: Промпт -> Замороженная модель -> Ответ.

### Слайд 4.2: Вероятностный хаос (Temperature)
*   **Заголовок:** Почему ИИ всегда разный?
*   **Текст:**
    *   Модель не выбирает «правильный» ответ. Она выбирает «вероятный».
    *   **Temperature:** Параметр, который разрешает модели «рисковать» и выбирать менее вероятные слова (креативность vs точность).
    *   Вот почему на один и тот же вопрос вы получаете разные ответы.
*   **Иллюстрация:** Столбчатая диаграмма вероятностей для следующего слова.

---

## Финал: Открытый вопрос

### Слайд 5.1: Зеркало человечества
*   **Заголовок:** Инструмент или Интеллект?
*   **Текст:**
    *   ИИ съел всё, что написали люди. Он — наше коллективное отражение.
    *   **Вопрос для обсуждения:** Если модель предсказывает «только следующее слово», значит ли это, что она не понимает смысл? 
    *   Или понимание — это и есть очень точное предсказание?
*   **Иллюстрация:** Философское изображение зеркала или лабиринта.
