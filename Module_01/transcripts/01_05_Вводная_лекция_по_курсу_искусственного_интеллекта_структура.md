01-05 Вводная лекция по курсу искусственного интеллекта: структура, основы и экосистема моделей
Вводная лекция по курсу искусственного интеллекта: структура, основы и экосистема моделей

В первой встрече преподаватель Николай Сехачев обозначил замысел курса и формат работы: знакомство с участниками, обзор учебной программы и практического проекта, затем фундаментальная лекция о принципах нейросетей — от биологических аналогий к математическим моделям — с акцентом на сложность и стоимость обучения современных систем, и, наконец, обзор экосистемы моделей, включая мультимодальность и реальные инструменты для работы.

Введение и знакомство с участниками

Преподаватель задает тон занятия: сначала обсуждение программы и знакомство в тесном, диалоговом формате, затем три академических часа лекции, где поровну уделяется общим принципам работы ИИ/моделей/нейросетей и обсуждению того, как развитие технологий меняет прикладной бизнес-ландшафт. Призывает к “зум-ауту” — взгляду на технологию с высоты, чтобы понять, как встраивать знания в реальные процессы.

Николай Сехачев представляется: образование — астрофизика (Екатеринбург), затем юриспруденция, далее MBA (США). Преподавал руководителям и собственникам бизнеса в Стокгольмской школе экономики в Риге. Позиционирует себя не со стороны IT, а со стороны клиентской, понимая бизнес-потребности и рассматривая технологии через призму задач. Подчеркивает философию курса: фундаментальность, доступное объяснение сложного материала, диалоговый режим, акцент на прикладную пользу и связку с бизнесом. Отмечает работу с Балтик Бизнес Клубом и признание “Визионер года” за внедрение решений.

Участники:
Ольга (поток SMM): инженер без опыта работы, занимается контентом как контент-креатор — генерация идей, съемка, продажи. Использует ChatGPT выборочно и критически, не полагается полностью, воспринимает его как источник идей. Пробовала обучающие материалы по генерации контента, но решила начать с этим курсом.
Татьяна (поток автоматизации): два высших образования (инженер-технолог; логист-экономист). Хочет углубиться в сферу ИИ, понять возможности заработка и реализации идей, использует GPT для простых вопросов, экспериментирует с формулировками запросов, чтобы получить оптимальный ответ. Николай фиксирует договоренность: идеи не “замыливать”, обсуждать и развивать в практической части.
Андрей: доктор экономических наук, педагог с 26-летним стажем, организатор образовательного процесса. Во второй половине 2024-го и в 2025-м изучал ИИ на двухдневных курсах/семинарах. Хочет уйти “вглубь”, так как короткие тренинги часто посвящены продвижению больших продуктов и дают “вершки”. Практиковал создание контента к выборам с помощью Grok и Gemini, меньше — ChatGPT (“врет”). Достигает 4 тыс. прочтений для бытовых тем и 50–60 тыс. для социальных тем.
Олена (поток SMM): многолетний опыт руководства интернет-магазином (операционка, маркетинг, digital-маркетинг, от построения сайта до развития), умеет писать тексты, делать видео, формировать контент, ставить задачи и делать финансовые отчеты (первое образование — финансист). С ИИ знакома хорошо, но в SMM применяла только для видео, фото и контент-планов.
Преподаватель отмечает компактность группы, призывает к активным вопросам и диалогу.

Структура, логистика и философия курса

Философия: дать “удочку”, а не “рыбу” — обучить фундаментальному пониманию, чтобы студенты могли адаптироваться при смене инструментов и моделей. Поясняется существование двух миров IT: платных пользовательских сервисов и open-source решений. Последние требуют преодоления технологического барьера (“кирпичная стена”), чему курс поможет.

Структура программы:
Общие фундаментальные модули (с 1 по 6): как устроены ИИ/нейросети; углубление в языковые модели; распознавание и синтез речи (важно для автоматизации бизнеса и контента); генерация изображений и компьютерное зрение (включая OCR — полезно для SMM и документов).
Переход на более продвинутый уровень: создание автоматических решений без постоянного написания промптов.
Инструменты для автоматизации:
  Nathan (M8M) — no-code для визуального построения сценариев выполнения задач.
Заключительный модуль — low/white-coding: использование ассистента для написания кода и построения решений на базе кода.
После общей части — специальные форматы встреч по трекам (SMM и автоматизация).
  Автоматизация: построение чат-ботов, использование баз данных, разработка агентов.
SMM: генерация видео, работа с соцсетями для автоматического постинга, создание контент-конвейеров.

Организация:
Формат: оффлайн-встречи, домашние задания, онлайн-уроки с разбором проблем/результатов; личный контакт оффлайн особо важен.
Каналы связи: планируется Telegram-группа для оперативного общения и доступа к материалам (презентации).
Тайминг: встречи два модуля в неделю; предложенный график — понедельник и четверг в 9:00. Такой ритм позволяет выделять время на проекты во вторник и среду, а в четверг разбирать результаты.
Общее число модулей — десять; при двух модулях в неделю — пять недель обучения; завершение курса — первая неделя февраля.
Нагрузка и результаты: по замечанию Андрея, программа предполагает 80 академических часов контактных занятий и 40 часов самостоятельных проектов; финал — MVP-проект (прототип решения/продукта), согласуемый с Андреем и социальным фондом для пользы. Критерии успеха: посещаемость 80%, выполнение всех микропроектов, защита MVP. По успешному завершению — сертификат и портфолио.
Студенты могут выбирать формат присутствия, но рекомендуется оффлайн для лучшего взаимодействия.

Основы нейросетей: от биологии к математической модели

Контекст восприятия ИИ: у IT-специалистов появление ChatGPT вызвало шок из-за уровня понимания текста и генерации связных ответов с улавливанием интонаций; для массового пользователя эффект неожиданности был меньше, поскольку за последние 10 лет многие сталкивались с ML-продуктами (например, Prisma в 2018, стилизация изображений). Как сказал Артур Кларк: “любая достаточно продвинутая технология неотличима от магии”.

Сравнение подходов: традиционное программирование — последовательности строгих инструкций (пример — код программы “Аполлон”: Маргарет Хамильтон, 145 тысяч строк, ошибка недопустима). Но такой подход не решает задачи распознавания образов (например, “кто на картинке: собака или кошка?”) из-за огромного разнообразия вариантов и ракурсов, которое невозможно исчерпывающе описать правилами.

Биология и модель:
Базовая единица мозга — нейрон: принимает информацию через дендриты, обрабатывает, передает через аксон; связи — синапсы. Сложность и “магия” возникают при огромном числе клеток и связей.
Математический аналог — перцептрон: на вход приходят сигналы с чувствительностями (весами), суммируются; при превышении порога нейрон активируется и передает сигнал дальше — простая функция. Сеть из многих таких узлов формирует входной слой, скрытые слои и выходной слой; связи имеют веса.
Алгоритм классический — детерминированный: одинаковые входы → одинаковый выход. Нейросети дают вероятностную картину — набор решений с оценками вероятности.

Пример: крестики-нолики
Поле представлено девятью входами; состояние клетки: пусто (0), крестик (1), нолик (-1).
На выходе — 9 нейронов с вероятностями выбора хода; один обычно близок к 1, остальные к 0.

Распознавание и генерация:
Вход может быть текст, картинка, звук, голос, видео; выход — список вероятностей.
В изображениях нейросеть “загорает” нейрон ключа (“собака”, “кошка”, “дом”, “человек”) с вероятностью (например, “собака” на 99%). Распознавание рукописного текста — тоже подача изображения в сеть.
Генерация изображений — обратная задача: по тексту создается картинка (пример — обложка Cosmopolitan 2023: “женщина-астронавт, шагает по планете в розовом, Synthwave”).
Языковая модель предсказывает следующее слово/токен за один проход по словарю, выдавая распределение вероятностей (например, после “кот” вероятнее “молоко”, а не “деньги”).

Сложность, обучение и стоимость AI-моделей

Масштабы:
Простые модели (крестики-нолики) для осмысленной игры: порядка 30 тысяч параметров.
GPT-3: 175 миллиардов параметров.
GPT-4: порядка трех миллионов параметров.
Оценка масштабов времени: миллион секунд ≈ 12 суток; миллиард секунд ≈ 32 года.

Ресурсы и стоимость:
Обучение Llama 3.2: аренда дата-центра с 10 тысячами GPU на 10 дней, стоимость примерно полмиллиона долларов; результат — файл около 360 ГБ с архитектурой и весами.
Для хороших моделей нужны данные (качественные, подготовленные, размеченные), ресурсы (GPU) и люди (архитекторы нейросетей).
Nvidia — производитель GPU — за 10 лет выросла в стоимости в 600 раз и стала одной из самых дорогих компаний; стоимость 5 триллионов долларов.

Запуск и квантизация:
Вес каждого файлика модели Kwent (уровня лидеров) — 4 ГБ; вся модель может весить до 500 ГБ.
Для запуска модели весом 500 ГБ требуется 500 ГБ видеопамяти; топовые пользовательские видеокарты имеют 24–32 ГБ.
Квантизация (округление точности весов) уменьшает объем модели (например, в 10 раз), снижая точность, но сохраняя работоспособность для многих задач. Для практики не всегда нужна “супер-хорошая” модель — достаточно проще и быстрее.

Классификация и экосистема AI-моделей

Переход от LLM к фундаментальным моделям: современные системы становятся мультимодальными и умеют воспринимать и генерировать текст, изображения, голос, видео без обязательных промежуточных преобразований. Если раньше голос → STT → текст → LLM → TTS → голос, то теперь возможна голос-в-голос обработка в реальном времени.

Примеры и форматы:
Текст→3D: модели выдают 3D-объекты по тексту.
Видеоаналитика: выделение объектов на видеопотоке (машины, люди).
Text→Video: по запросу “зеленая черепаха ползет по пляжу” генерируется видео.
Внутри текстовой модальности — специализированные задачи: токенизация (преобразование текста в вектора), эмбеддинги, классификация отзывов (позитив/негатив, ключевые слова).
Text-to-Speech (TTS) — из текста в аудио; Suno известен как генератор музыки/песен.
Speech-to-Text (STT) — распознавание речи; Large Language Model (LLM) — языковая модель.

Стриминг и реальное время:
Стриминг снижает задержки: параллельно транскрибировать речь, передавать текст в LLM, и как только LLM начинает отвечать — сразу подавать первое слово в TTS. Экономия — примерно до одной секунды.
Google Gemini (Native Audio Preview): мультимодальная модель, умеет говорить в реальном времени и высылает функции. Lip-sync — синхронизация движения губ с аудио.

Открытая экосистема:
Hugging Face — репозиторий открытых моделей (текст, голос, музыка и др.). Открытая культура впечатляет: производители публикуют результаты многолетней работы.
Архитектура трансформера (Attention is all you need, Google, 2018) лежит в основе многих моделей; OpenAI использует эту схему. Alibaba (Qwen) — модели уровня лидеров, входят в десятку.

Action Items

@Николай Сехачев
   Создать группу в Telegram для общения и обмена материалами - [TBD]
 Выложить презентации или предоставить доступ к ним в Telegram-группе - [TBD]
 Настроить ссылку для онлайн-урока - [TBD]
 Сходить на Hugging Face, поизучать и попробовать разные модели (включая Spaces), чтобы понять, что такое модель и как её запускать - [TBD]
